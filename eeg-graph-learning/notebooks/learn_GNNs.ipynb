{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a GCN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Envrionment setup\n",
    "\n",
    "Before you start please run `setup_conda_env.sh` first. You will need to make it executable and run it (check README , same process for clean.sh)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import networkx as nx\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is a citation network. For a given paper, it has information about which papers reference it and which papers it references\n",
    "\n",
    "In this data, we represent each paper as a node. Each node has an ID and the paper is represented as a bag of words (each column represents the presence or absence of a word in the paper).\n",
    "\n",
    "Each paper is 1 of 7 classes (https://graphsandnetworks.com/the-cora-dataset/)\n",
    "\n",
    "**It's not the case that every paper is cited, but every paper cites atleast one other paper in the dataset : the graph is fully connected**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('31336', 'Neural_Networks\\n')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load papers\n",
    "# Here, each  line is a paper\n",
    "with open('cora/cora.content') as file:\n",
    "    papers = file.readlines()\n",
    "# This will be loaded tab seperated, so let's remove the tabs\n",
    "# and get a list of papers\n",
    "papers = [p.split('\\t') for p in papers]\n",
    "# The first element is the paper ID and the last is the group classification\n",
    "# so lets get those\n",
    "ids = [p[0] for p in papers]\n",
    "labels = [p[-1] for p in papers]\n",
    "ids[0], labels[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the labels\n",
    "# use set to get unique values\n",
    "# check the raw strings\n",
    "for l in set(labels):\n",
    "    print(repr(l))\n",
    "# remove the new line characters\n",
    "labels = [l.strip() for l in labels]\n",
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, each column is the presence or absence of a word in the paper\n",
    "# the first column is the paper ID and the last column is the label\n",
    "for i in range(5):\n",
    "    print(papers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the citation data\n",
    "with open('cora/cora.cites') as file:\n",
    "    cites = file.readlines()\n",
    "\n",
    "# check the raw strings\n",
    "# this shows A cites B as A \\tab B\n",
    "print(repr(cites[0]))\n",
    "#  remove the new line, make the links sublists\n",
    "cites = [c.strip().split('\\t') for c in cites]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i,j] means j cites i. \n",
    "# This only has the incoming edges to all the papers\n",
    "# So here  paper 1033 cites paper 35, etc.\n",
    "cites[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the graph representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make them dataframes for easy manipulation\n",
    "papers_df = pd.DataFrame(papers)\n",
    "\n",
    "papers_df[papers_df.columns[0:1433]] = papers_df[papers_df.columns[0:1433]].astype(int)\n",
    "# Let's call  papers X and remove the ID and label\n",
    "# Each row represents a node\n",
    "X = papers_df.drop(columns = [0,1434])      \n",
    "# do integer encoding of the text labels : map the labels to integers\n",
    "le = LabelEncoder()\n",
    "y  = le.fit_transform(labels)\n",
    "papers_df.head()\n",
    "# features          \n",
    "print(X.head())\n",
    "# labels\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The adjacency matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjacency matrix is a square matrix that tells us how the nodes in a graph are connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many papers are there ?\n",
    "N_papers = papers_df.shape[0]\n",
    "# a dictionary for reindexing \n",
    "# Why ? Because these papers are a sample of a bigger dataset, so the paper IDs are not contiguous.\n",
    "# so we map the paper IDs to contiguous integers\n",
    "to_new_index = {int(paper_id) : index for index, paper_id in enumerate(papers_df[0])}\n",
    "index_to_paper = { int(index) : int(paper_id) for paper_id, index in to_new_index.items()}\n",
    "\n",
    "# map all the cites to the new index\n",
    "cites_new_index = [[to_new_index[int(cite[0])], to_new_index[int(cite[1])]] for cite in cites]\n",
    "\n",
    "for c in cites_new_index[:5]:\n",
    "    print(f\"paper {c[1]} --cites--> paper {c[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create a matrix representation for our adjacency information.\n",
    "# what we want is something like this:\n",
    "# where each column represents a citing paper and each row represents a cited paper.\n",
    "#\n",
    "#         Citing Paper â†’\n",
    "#        0    1    2    3    4\n",
    "#      +-------------------------+\n",
    "#  0  |  0    0    0    1    0  |  <-- Paper 0 is cited by Paper 3 \n",
    "#  1  |  1    0    0    0    0  |  <-- Paper 1 is cited by Paper 0 \n",
    "#  2  |  0    1    0    0    0  |  <-- Paper 2 is cited by Paper 1 \n",
    "#  3  |  1    0    1    0    0  |  <-- Paper 3 is cited by Paper 0 and Paper 2 \n",
    "#  4  |  0    0    0    0    0  |  <-- Paper 4 is not cited by any paper\n",
    "#      +-------------------------+\n",
    "#\n",
    "# In this matrix:\n",
    "# - The columns represent the citing papers.\n",
    "# - The rows represent the cited papers.\n",
    "# - A value of 1 at entry (r, c) indicates that paper c cites paper r.\n",
    "#\n",
    "\n",
    "# If we have a very larg network, having a massive matrix with a lot of 0s is unncessary\n",
    "# https://youtu.be/Qi7FcjN7nsc?si=3_lKyS6MlgD2IWKy\n",
    "# We make a csr-representation. Instead of holding the full matrix, we maintain  information that\n",
    "# will defines it.\n",
    "\n",
    "import scipy.sparse as sp\n",
    "# turn the cites into a numpy array:\n",
    "cites_new_index_arr = np.array(cites_new_index)\n",
    "\n",
    "# cites_new_index_arr looks like:\n",
    "# [[cited, citer],\n",
    "#  [cited, citer],\n",
    "#  ......]\n",
    "# We get the cited papers : all the values in the first column\n",
    "row =  cites_new_index_arr[:,0]\n",
    "# We get the citing papers : all the values in the second column\n",
    "col = cites_new_index_arr[:,1]\n",
    "# We want to place a one at each [cited, citer] element\n",
    "values = np.ones(len(col)) \n",
    "# This puts a 1 in every [cited, citer] element, holding only these non-zero elements which conserves space.\n",
    "A  = sp.csr_matrix((values,(row,col)), shape=(N_papers,N_papers))\n",
    "A = csr_matrix(A)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It still has the same shape technically, but it is a sparse matrix\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "import random\n",
    "# Add edges: the edge is directed from \"citing\" to \"cited\"\n",
    "# lets look at 15 edges\n",
    "for cited, citing in cites_new_index[200:215]:\n",
    "    G.add_edge(citing, cited)\n",
    "# Generate a layout for our nodes (here we use a circular layout)\n",
    "pos = nx.circular_layout(G)\n",
    "\n",
    "# Draw nodes, edges, and labels\n",
    "plt.figure(figsize=(8, 8))\n",
    "nx.draw_networkx_nodes(G, pos, node_color='skyblue', node_size=700)\n",
    "nx.draw_networkx_edges(G, pos, arrowstyle='->', arrowsize=20, edge_color='gray')\n",
    "nx.draw_networkx_labels(G, pos, font_size=12, font_color='black')\n",
    "\n",
    "plt.title(\"Citation Network Visualization\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The node degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Node degree\n",
    "# The node degree answers the question : how many connections does a given node have?\n",
    "\n",
    "# We add self-loops because we will need this for the normalization step coming next\n",
    "A  += np.diag(np.ones(N_papers))\n",
    "degrees = []\n",
    "for i in range(A.shape[0]): # for each paper\n",
    "    incoming = np.sum(A[i,:])  # Papers that cite paper i (incoming edges)\n",
    "    outgoing = np.sum(A[:,i])  # Papers that paper i cites (outgoing edges)\n",
    "    # This counts all connections to a given paper both 'cited by' (rows) and 'is citing' (columns)\n",
    "    degrees.append(outgoing + incoming)\n",
    "    if outgoing + incoming < 1.0:\n",
    "        print(f\"Paper {i} ({index_to_paper[i]}) has no connections\")\n",
    "\n",
    "# We create a matrix D which is a diagonal matrix (only the main diagonal has non-zero values).\n",
    "# This represents the edges that a given paper has\n",
    "# We could also make it a sparse representation but we skip this for now\n",
    "D = np.diag(degrees)\n",
    "adj_matrix = csr_matrix(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(D)\n",
    "print(D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check\n",
    "#  paperID of index 0\n",
    "p = index_to_paper[0]\n",
    "print(p)\n",
    "for c in cites:\n",
    "    if c[0] == str(p) or c[1] == str(p):\n",
    "        print(c)\n",
    "\n",
    "#  This is correct. There are 5 edges total but beause we added self loops , we see 7 in the D matrix above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, we have the following important coponents\n",
    "# X : These are the node features \n",
    "#   : This has shape N_papers x BOW features\n",
    "print(\" X shape : \", X.shape)\n",
    "\n",
    "# A : This is the adacency matrix. It contains the information about which papers are connected to each other\n",
    "#   : This has shape N_papers x N_papers\n",
    "print(\" A shape : \", adj_matrix.shape)\n",
    "\n",
    "# D : This is the node degree matrix. It contains the information about how many incoming and outgoing edges each paper has\n",
    "#   : This has shape N_papers x N_papers\n",
    "print(\" D shape : \", D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Graph convolution layer (https://arxiv.org/pdf/1609.02907) applies this function:\n",
    "$$\n",
    "H^{(l+1)} = \\sigma \\left( \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} H^{(l)} W^{(l)} \\right)\n",
    "$$\n",
    "\n",
    "That is, the node features (H) are updated with the connectivity information of each of the nodes in the network (provided by A and D) and W (learnable weights). The learnable weights serve the same role as in a standard neural network where the output of a given layer is  $a = \\sigma(WX+b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # convert everything to tensors, and rename to match the conventions\n",
    "X_tensor = torch.Tensor(X.astype(int).to_numpy())\n",
    "A_tensor = torch.sparse_csr_tensor(\n",
    "                    torch.from_numpy(adj_matrix.indptr),\n",
    "                    torch.from_numpy(adj_matrix.indices),\n",
    "                    torch.from_numpy(adj_matrix.data))\n",
    "D_tensor = torch.Tensor(D)\n",
    "\n",
    "# The labels assigned to a given paper from the original dataset\n",
    "y_tensor= torch.Tensor(y)\n",
    "\n",
    "# Let'compute the inverse square root of D\n",
    "# Get the diagonal elements of D\n",
    "D_diag = torch.diag(D_tensor)\n",
    "# Compute the inverse square root\n",
    "D_inv_sqrt = 1 / torch.sqrt(D_diag)\n",
    "# Create a diagonal matrix from the inverse square root\n",
    "D_inv_sqrt_diag = torch.diag(D_inv_sqrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's now build this operation out in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define a GCN layer\n",
    "class GCNlayer(nn.Module):\n",
    "    def __init__(self, X_in_shape : int, X_out_shape : int,  D_inv_sqrt_diag : torch.Tensor, A : torch.Tensor) -> None:\n",
    "        super(GCNlayer, self).__init__()\n",
    "        # Since we want to do X @ W, we need to make sure the dimensions match\n",
    "        # X has shape N_papers x BOW features\n",
    "        # W has shape BOW features x W_cols : We can adjust the columns to change the output size\n",
    "        self.W_rows = X_in_shape\n",
    "        self.W_cols = X_out_shape\n",
    "        self.W = nn.Linear(in_features=self.W_rows, out_features=self.W_cols)\n",
    "        self.D_inv_sqrt_diag = D_inv_sqrt_diag\n",
    "        self.A = A\n",
    "    def forward(self, X : torch.Tensor) -> torch.Tensor:\n",
    "        X_updated = self.W(X) # X @ W\n",
    "        X_updated = self.D_inv_sqrt_diag @ X_updated # D^(-1/2) @ X @ W\n",
    "        X_updated = self.A @ X_updated # A @ D^(-1/2) @ X @ W\n",
    "        # Returns the transformed X of shape N_papers x W_cols\n",
    "        X_updated = self.D_inv_sqrt_diag @ X_updated\n",
    "        return X_updated # shape N_papers x W_cols\n",
    "\n",
    "# We can have any number of layers we want.\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self,X_in_shape  : int, layer_out_sizes : list[int], D_inv_sqrt_diag : torch.Tensor, A : torch.Tensor) -> None:\n",
    "        super(GCN, self).__init__()\n",
    "        # first layer\n",
    "        self.layer1 = GCNlayer(X_in_shape= X_in_shape, # = 1433\n",
    "                               X_out_shape= layer_out_sizes[0], # = 100\n",
    "                               D_inv_sqrt_diag= D_inv_sqrt_diag,\n",
    "                               A= A)\n",
    "        # second layer\n",
    "        self.layer2 = GCNlayer(X_in_shape= layer_out_sizes[0], # The input shape of the second layer is the output shape of the first layer\n",
    "                               X_out_shape= layer_out_sizes[1], # = 7\n",
    "                               D_inv_sqrt_diag= D_inv_sqrt_diag,\n",
    "                               A= A)\n",
    "        # softmax layer to get the probability distribution over the classes\n",
    "        # returns a tensor of shape N_papers x 7\n",
    "        self.softmax = nn.Softmax(dim= 1)\n",
    "    def forward(self, X):\n",
    "        X = torch.relu(self.layer1(X))\n",
    "        X = self.layer2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure both tensors are of the same type (Float or Double)\n",
    "X_tensor = X_tensor.to(torch.float32)  # Convert X_tensor to Float\n",
    "A_tensor = A_tensor.to(torch.float32)  # Convert A_tensor to Float\n",
    "\n",
    "l = GCNlayer(X_in_shape = X_tensor.shape[1], # This is the number of features in the input node features = BOW features = 1433 \n",
    "              # This is the number of features in the output node features = 100. We can choose this arbitrarily. \n",
    "              # The output of one layer will be of shape X_previous_layer x 100.\n",
    "             X_out_shape = 100,\n",
    "             D_inv_sqrt_diag = D_inv_sqrt_diag,\n",
    "             A = A_tensor)\n",
    "l.forward(X_tensor).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the network\n",
    "net = GCN(\n",
    "    X_in_shape=X_tensor.shape[1], # input shape = N_papers x BOW features\n",
    "    layer_out_sizes=[100, 7], # first layer has 100 features, second layer has 7 features\n",
    "    D_inv_sqrt_diag=D_inv_sqrt_diag,\n",
    "    A=A_tensor\n",
    ")\n",
    "# We should expect the output to be of shape N_papers x layer_out_sizes after each layer\n",
    "# and the final output to be of shape N_papers x 7\n",
    "net(X_tensor).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a network to predict the label for a given node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall that we 7 classes \n",
    "print(set(labels))\n",
    "# encoded as integers\n",
    "print(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's separate the data into training and test sets\n",
    "# Create boolean masks for training, validation, and testing  with 60% for training, 20% for validation, and 20% for testing\n",
    "# use train test split to get the indices\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_indices, val_test_indices = train_test_split(range(2708), test_size=0.4, random_state=42)\n",
    "val_indices, test_indices = train_test_split(val_test_indices, test_size=0.5, random_state=42)\n",
    "# use the indices to get the masks\n",
    "train_mask = torch.zeros(2708, dtype=torch.bool)\n",
    "train_mask[train_indices] = True\n",
    "\n",
    "val_mask = torch.zeros(2708, dtype=torch.bool)\n",
    "val_mask[val_indices] = True\n",
    "\n",
    "test_mask = torch.zeros(2708, dtype=torch.bool)\n",
    "test_mask[test_indices] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can build and train our network\n",
    "net = GCN(X_in_shape=X_tensor.shape[1], # input shape = 1433\n",
    "          layer_out_sizes=[100, 7], # first layer has 100 features, second layer has 7 features\n",
    "          D_inv_sqrt_diag=D_inv_sqrt_diag,\n",
    "          A=A_tensor)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that only the W matrices are trainable\n",
    "# Pytorch will automatically set the requires_grad attribute to True for all parameters by default\n",
    "# Named parameters returns a list of tuples, where each tuple contains the name of the parameter and the parameter itself\n",
    "# We can use this to check which parameters are trainable\n",
    "# We should expect only the W matrices to have requires_grad =  True\n",
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can do a standard training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "for epoch in range(150):\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Each forward pass computes the output for all nodes in the network\n",
    "    output = net(X_tensor)\n",
    "    # We only want to compute the loss for the training nodes\n",
    "    loss = criterion(output[train_mask], y_tensor[train_mask].long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        output = net(X_tensor)\n",
    "        loss = criterion(output[val_mask], y_tensor[val_mask].long())\n",
    "        if epoch % 10 == 0:\n",
    "            val_outputs = net(X_tensor)\n",
    "            val_accuracy = (val_outputs[val_mask].argmax(1) == y_tensor[val_mask]).float().mean()\n",
    "            print(f\"Validation Accuracy: {val_accuracy.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the network\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    output = net(X_tensor)\n",
    "    test_loss = criterion(output[test_mask], y_tensor[test_mask].long())\n",
    "    test_accuracy = (output[test_mask].argmax(1) == y_tensor[test_mask]).float().mean()\n",
    "    print(f\"Test Accuracy: {test_accuracy.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg-graph-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
